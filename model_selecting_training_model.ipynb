{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600348223025",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def file_exists(filepath):\n",
    "    if os.path.exists(filepath):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def rename_filename(filepath, number=1):\n",
    "    name = os.path.splitext(filepath)[0]\n",
    "    extension = os.path.splitext(filepath)[1]\n",
    "\n",
    "    name += \"_\" + str(number)\n",
    "    filepath = name + extension\n",
    "    if file_exists(filepath):\n",
    "        return rename_filename(filepath=filepath, number=number+1)\n",
    "    else:\n",
    "        return filepath\n",
    "\n",
    "def save_model(to_save, filepath):\n",
    "    import pickle\n",
    "\n",
    "    try:\n",
    "        if file_exists(filepath=filepath):\n",
    "            filepath = rename_filename(filepath=filepath)\n",
    "        pickle.dump(to_save, open(filepath, 'wb'))\n",
    "        print(\"Saved successfully\")\n",
    "        return True, filepath\n",
    "    except Exception as e:\n",
    "        print(\"Error during saving model:\\n\", e)\n",
    "        return False, filepath\n",
    "\n",
    "\n",
    "def choose_model(option_user, **params):\n",
    "    if int(option_user) == 1:\n",
    "        model_option = input(\"Which model do you want to use?                                                                              1 = LinearRegression 2 = PolynomialFeatures 3 = SVM - SVR                                                    4 =  RandomForestRegressor\")\n",
    "        option_user = int(model_option)\n",
    "        if option_user == 1:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = LinearRegression(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = LinearRegression()\n",
    "                return model\n",
    "\n",
    "        if option_user == 2:\n",
    "            from sklearn.preprocessing import PolynomialFeatures\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = PolynomialFeatures(**v) \n",
    "                return model\n",
    "            else:\n",
    "                raise ValueError(\"Missing argument degree\")\n",
    "            \n",
    "        if option_user == 3:\n",
    "            from sklearn.svm import SVR\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = SVR(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = SVR()\n",
    "                return model\n",
    "\n",
    "        if option_user == 4:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = RandomForestRegressor(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = RandomForestRegressor()\n",
    "                return model\n",
    "\n",
    "        return model \n",
    "        \n",
    "    elif int(option_user) == 2:\n",
    "        model_option = input(\"Which model do you want to use? 1 = LogisticRegression, 2 = svm - SVC, 3 =                                  KNeighborsClassifier 4 = RandomForestClassifier(), 5 = XGBClassifier()\")\n",
    "        \n",
    "        option_user = int(model_option)\n",
    "\n",
    "        if option_user == 1:\n",
    "            from sklearn.linear_model import LogisticRegression\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = LogisticRegression(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = LogisticRegression()\n",
    "                return model\n",
    "\n",
    "\n",
    "        if option_user == 2:\n",
    "            from sklearn import svm\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = svm.SVC(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = svm.SVC()\n",
    "                return model\n",
    "\n",
    "        if option_user == 3:\n",
    "            from sklearn.neighbors import KNeighborsClassifier\n",
    "            for k,v in params.items():\n",
    "                model = KNeighborsClassifier(**v) \n",
    "                return model\n",
    "            else:\n",
    "                raise ValueError(\"Missing argument n_neighbors\")\n",
    "\n",
    "        if option_user == 4:\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            if params:\n",
    "                for k,v in params.items():\n",
    "                    model = RandomForestClassifier(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = RandomForestClassifier()\n",
    "                return model\n",
    "\n",
    "        if option_user == 5:\n",
    "            from xgboost import XGBClassifier\n",
    "            for k,v in params.items():\n",
    "                model = XGBClassifier(**v) \n",
    "                return model\n",
    "            else:\n",
    "                model = XGBClassifier()\n",
    "                return model\n",
    "\n",
    "#return model\n",
    "\n",
    "def train_model(model, df, target_name):\n",
    "    X = df.drop(target_name, 1).values\n",
    "    y = df[target_name].values\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=4)\n",
    "    print(X_train)\n",
    "    \n",
    "    kfold_train = input(\"Do you want cross validation? yes or no\")\n",
    "    if kfold_train.lower() != \"yes\":\n",
    "        \"\"\"\n",
    "    Starting training process with all X_train data\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "        if str(model).startswith(\"PolynomialFeatures\"):\n",
    "            \n",
    "            X_poly = model.fit_transform(X_train, y_train)\n",
    "\n",
    "            many_linear = (input('Enter the number of parameters you want to give for LinearRegression, if you dont want any, enter: no. '))\n",
    "            param_list_linear = {}\n",
    "            if many_linear.lower == \"no\":\n",
    "                lin_reg_model = LinearRegression()\n",
    "            else:\n",
    "                for i in range(int(many_linear)):\n",
    "                    data = input('Enter parameter & value separated by \":\" ') \n",
    "                    temp = data.split(':') \n",
    "                    if temp[1].isdigit():\n",
    "                        param_list_linear[temp[0]] = int(temp[1]) \n",
    "                    elif (\"True\" in temp[1])|(\"False\" in temp[1]):\n",
    "                        param_list_linear[temp[0]] = bool(temp[1])\n",
    "                    else:\n",
    "                        param_list_linear[temp[0]] = temp[1]\n",
    "                if param_list_linear:\n",
    "                    lin_reg_model = LinearRegression(**param_list_linear) \n",
    "                    \n",
    "\n",
    "            model_trained = lin_reg_model.fit(X_poly, y_train)\n",
    "\n",
    "            X_test_poly = model.fit_transform(X_test, y_test)\n",
    "            accuracy = model_trained.score(X_test_poly, y_test)\n",
    "\n",
    "        else:\n",
    "            model_trained = model.fit(X_train, y_train)\n",
    "            accuracy = model_trained.score(X_test, y_test)\n",
    "    else:\n",
    "        small_portions = input(\"Do you want to use cross validation in small steps? (for large datasets), put yes or no.\")\n",
    "        if small_portions == \"no\":\n",
    "            \"\"\"\n",
    "            Starting the training with cross validation normally\n",
    "            \"\"\"\n",
    "            if str(model).startswith(\"PolynomialFeatures\"):\n",
    "                    n_splits = int(input(\"Put number of n_splits:\"))\n",
    "                    n_repeats = int(input(\"Put the number of n_repeats:\"))\n",
    "                    k_fold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=4)\n",
    "                    val_score = []\n",
    "                    train_score = []\n",
    "                    \n",
    "                    X_poly = model.fit_transform(X_train, y_train)\n",
    "\n",
    "                    many_linear_1 = input('Enter the number of parameters you want to give for LinearRegression, if you dont want any, enter: no. ')\n",
    "                    \n",
    "                    param_list_linear = {}\n",
    "                    if many_linear_1 == \"no\":\n",
    "                        \n",
    "                        lin_reg_model = LinearRegression()\n",
    "                    else:\n",
    "                        for i in range(int(many_linear_1)):\n",
    "                            data = input('Enter parameter & value separated by \":\" ') \n",
    "                            temp = data.split(':') \n",
    "                            if temp[1].isdigit():\n",
    "                                param_list_linear[temp[0]] = int(temp[1]) \n",
    "                            elif (\"True\" in temp[1])|(\"False\" in temp[1]):\n",
    "                                param_list_linear[temp[0]] = bool(temp[1])\n",
    "                            else:\n",
    "                                param_list_linear[temp[0]] = temp[1]\n",
    "                        if param_list_linear:\n",
    "                            lin_reg_model = LinearRegression(**param_list_linear) \n",
    "                            \n",
    "                    for i, (train, val) in enumerate(k_fold.split(X_poly)):\n",
    "                        model_trained = lin_reg_model.fit(X_poly[train], y_train[train])\n",
    "                        score_val = lin_reg_model.score(X_poly[val], y_train[val])\n",
    "                        val_score.append(score_val)\n",
    "                        score_train = lin_reg_model.score(X_poly[train], y_train[train])\n",
    "                        train_score.append(score_train)\n",
    "                    #model_trained = lin_reg_model.fit(X_poly, y_train)\n",
    "\n",
    "                    X_test_poly = model.fit_transform(X_test, y_test)\n",
    "                    accuracy = model_trained.score(X_test_poly, y_test)\n",
    "            else:\n",
    "                n_splits = int(input(\"Put number of n_splits:\"))\n",
    "                n_repeats = int(input(\"Put the number of n_repeats:\"))\n",
    "                k_fold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=4)\n",
    "                val_score = []\n",
    "                train_score = []\n",
    "                for i, (train, val) in enumerate(k_fold.split(X_train)):\n",
    "                    model_trained = model.fit(X_train[train], y_train[train])\n",
    "                    score_val = model.score(X_train[val], y_train[val])\n",
    "                    val_score.append(score_val)\n",
    "                    score_train = model.score(X_train[train], y_train[train])\n",
    "                    train_score.append(score_train)\n",
    "\n",
    "                accuracy = model_trained.score(X_test, y_test)\n",
    "\n",
    "            \n",
    "            print(\"showing the learning process\")\n",
    "            plt.plot(train_score, label=\"train\")\n",
    "            plt.plot(val_score, label=\"val\", color=\"orange\")\n",
    "            plt.ylabel(\"score\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Starting cross validation with small steps - for large datasets. \n",
    "            \"\"\"\n",
    "            if str(model).startswith(\"PolynomialFeatures\"):\n",
    "                val_score = []\n",
    "                train_score = []\n",
    "                scores_small_trains = []\n",
    "                scores_small_vals = []\n",
    "                scores_smalls_trains_iterations = []\n",
    "                scores_smalls_vals_iterations = []\n",
    "\n",
    "                \n",
    "                n_splits = int(input(\"Put number of n_splits:\"))\n",
    "                n_repeats = int(input(\"Put the number of n_repeats:\"))\n",
    "                k_fold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=4)\n",
    "                n_small_splits = int(input(\"put number of n_splits for the small sections to train in cross validation\"))\n",
    "                kfold_small_trains = KFold(n_splits=n_small_splits, random_state=4)\n",
    "                \"\"\"\n",
    "                Per model the warm_state parameter is different (if they even excist). Look for documentation how to apply                        warm state for cross validation learning. below it will ask if you want to apply partial_fit\n",
    "                \"\"\"\n",
    "                partial_fit = input(\"apply partial_fit to model?, put yes or no\")\n",
    "    \n",
    "                \n",
    "                X_poly = model.fit_transform(X_train, y_train)\n",
    "\n",
    "                many_linear_1 = input('Enter the number of parameters you want to give for LinearRegression, if you dont want any, enter: no. ')\n",
    "                param_list_linear = {}\n",
    "                if many_linear_1 == \"no\":\n",
    "                    lin_reg_model = LinearRegression()\n",
    "                else:\n",
    "                    for i in range(int(many_linear_1)):\n",
    "                        data = input('Enter parameter & value separated by \":\" ') \n",
    "                        temp = data.split(':') \n",
    "                        if temp[1].isdigit():\n",
    "                            param_list_linear[temp[0]] = int(temp[1]) \n",
    "                        elif (\"True\" in temp[1])|(\"False\" in temp[1]):\n",
    "                            param_list_linear[temp[0]] = bool(temp[1])\n",
    "                        else:\n",
    "                            param_list_linear[temp[0]] = temp[1]\n",
    "                    if param_list_linear:\n",
    "                        lin_reg_model = LinearRegression(**param_list_linear) \n",
    "\n",
    "                for i, (train, val) in enumerate(k_fold.split(X_poly)):\n",
    "                    to_show_in_bar = \": \" + str(i) + \"/\" + str(n_splits * n_repeats)\n",
    "                    generator_val = kfold_small_trains.split(val)\n",
    "                    \n",
    "\n",
    "                    for i2,(_, small_train) in tqdm(enumerate(kfold_small_trains.split(train)), total=n_small_splits, desc=\"Small train  progress\" + to_show_in_bar):\n",
    "                        _, small_val = next(generator_val)\n",
    "                            \n",
    "\n",
    "                        if partial_fit.lower() == \"yes\":\n",
    "                            model_trained = lin_reg_model.partial_fit(X_poly[small_train], y_train[small_train], classes=np.unique(y))\n",
    "                        else:\n",
    "                            model_trained = lin_reg_model.fit(X_poly[small_train], y_train[small_train])\n",
    "\n",
    "                        score_small_train = model.score(X_poly[small_train], y_train[small_train])\n",
    "                        scores_smalls_trains_iterations.append(score_small_train)\n",
    "                        # val part\n",
    "                        score_small_val = model.score(X_poly[small_val], y_train[small_val])\n",
    "                        scores_smalls_vals_iterations.append(score_small_val)\n",
    "                    \n",
    "                        train_score.append(np.mean(scores_smalls_trains_iterations))\n",
    "                        scores_small_trains = scores_small_trains + list(scores_smalls_trains_iterations)\n",
    "                        scores_smalls_trains_iterations.clear()\n",
    "                        val_score.append(np.mean(scores_smalls_vals_iterations))\n",
    "                        scores_small_vals = scores_small_vals + list(scores_smalls_vals_iterations)\n",
    "                        scores_smalls_vals_iterations.clear()\n",
    "\n",
    "                    print(\"Iteration:\", to_show_in_bar, \"| Val_accuracy:\", np.mean(val_score),                                          \"| train_accuracy: \", np.mean(train_score), sep=\"~~~~~~\")\n",
    "                print(\"Trained finished!\")\n",
    "\n",
    "            else:\n",
    "                val_score = []\n",
    "                train_score = []\n",
    "                scores_small_trains = []\n",
    "                scores_small_vals = []\n",
    "                scores_smalls_trains_iterations = []\n",
    "                scores_smalls_vals_iterations = []\n",
    "\n",
    "                \n",
    "                n_splits = int(input(\"Put number of n_splits:\"))\n",
    "                n_repeats = int(input(\"Put the number of n_repeats:\"))\n",
    "                k_fold = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=4)\n",
    "                n_small_splits = int(input(\"put number of n_splits for the small sections to train in cross                                      validation\"))\n",
    "                \n",
    "                kfold_small_trains = KFold(n_splits=n_small_splits, random_state=4)\n",
    "                \"\"\"\n",
    "                Per model the warm_state parameter is different (if they even excist). Look for documentation how to apply                        warm state for cross validation learning. below it will ask if you want to apply partial_fit\"\"\"\n",
    "                partial_fit = input(\"apply partial_fit to model?, put yes or no\")\n",
    "\n",
    "                for i, (train, val) in enumerate(k_fold.split(X_train)):\n",
    "                    to_show_in_bar = \": \" + str(i) + \"/\" + str(n_splits * n_repeats)\n",
    "                    generator_val = kfold_small_trains.split(val)\n",
    "\n",
    "                    for i2,(_, small_train) in tqdm(enumerate(kfold_small_trains.split(train)), total=n_small_splits, desc=\"Small train  progress\" + to_show_in_bar):\n",
    "                        _, small_val = next(generator_val)\n",
    "                                \n",
    "\n",
    "                        if partial_fit.lower() == \"yes\":\n",
    "                            model_trained = model.partial_fit(X_train[small_train], y_train[small_train], classes=np.unique(y))\n",
    "                        else:\n",
    "                            model_trained = model.fit(X_train[small_train], y_train[small_train])\n",
    "\n",
    "                        score_small_train = model.score(X_train[small_train], y_train[small_train])\n",
    "                        scores_smalls_trains_iterations.append(score_small_train)\n",
    "                        # val part\n",
    "                        score_small_val = model.score(X_train[small_val], y_train[small_val])\n",
    "                        scores_smalls_vals_iterations.append(score_small_val)\n",
    "                        \n",
    "                        train_score.append(np.mean(scores_smalls_trains_iterations))\n",
    "                        scores_small_trains = scores_small_trains + list(scores_smalls_trains_iterations)\n",
    "                        scores_smalls_trains_iterations.clear()\n",
    "                        val_score.append(np.mean(scores_smalls_vals_iterations))\n",
    "                        scores_small_vals = scores_small_vals + list(scores_smalls_vals_iterations)\n",
    "                        scores_smalls_vals_iterations.clear()\n",
    "\n",
    "                        print(\"Iteration:\", to_show_in_bar, \"| Val_accuracy:\", np.mean(val_score), \"| train_accuracy: \",                                        np.mean(train_score), sep=\"~~~~~~\")\n",
    "                print(\"Trained finished!\")\n",
    "                    \n",
    "            accuracy = model_trained.score(X_test, y_test)\n",
    "            \n",
    "            print(\"showing the learning process\")\n",
    "            plt.plot(train_score, label=\"train\")\n",
    "            plt.plot(val_score, label=\"val\", color=\"orange\")\n",
    "            plt.ylabel(\"score\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "    return model_trained, accuracy\n",
    "\n",
    "'''\n",
    "for regression: \n",
    "    option 1 = LinearRegression\n",
    "    option 2 = PolynomialFeatures\n",
    "    option 3 = SVM - SVR\n",
    "    option 4 = RandomForestRegressor\n",
    "for classification: \n",
    "    option 1 = LogisticRegression\n",
    "    option 2 = KNeighborsClassifier\n",
    "    option 3 = svm - SVC\n",
    "    option 4 = RandomForestClassifier()\n",
    "    option 5 = XGBClassifier()\n",
    "'''\n",
    "def main(df):\n",
    "    choice = input(\"What type of problem: 1 for regression or 2 for classification?\")\n",
    "    params = input(\"Enter YES in case you want to enter a dictionary of params, if not neccesary put NO\") \n",
    "    target = input(\"What is the target column?\")\n",
    "\n",
    "    if params.lower() == \"no\":\n",
    "        model = choose_model(option_user=choice)\n",
    "        model_trained, accuracy = train_model(model=model, df=df, target_name=target)\n",
    "    else:\n",
    "        many = int(input('Enter the number of parameters you want to give: '))\n",
    "        param_list = {}\n",
    "        for i in range(many):\n",
    "            data = input('Enter parameter & value separated by \":\" ') \n",
    "            temp = data.split(':') \n",
    "            if temp[1].isdigit():\n",
    "                param_list[temp[0]] = int(temp[1]) \n",
    "            elif (\"True\" in temp[1])|(\"False\" in temp[1]):\n",
    "                param_list[temp[0]] = bool(temp[1])\n",
    "            else:\n",
    "                param_list[temp[0]] = temp[1]\n",
    "\n",
    "        model = choose_model(option_user=choice, params=param_list)\n",
    "        model_trained, accuracy = train_model(model=model, df=df, target_name=target)\n",
    "\n",
    "    import time\n",
    "    print(\"score of model:\", accuracy)\n",
    "    \n",
    "\n",
    "    time.sleep(3.5)    # pause 3.5 seconds\n",
    "\n",
    "    save = input(\"Do you want to save the model?, put yes or no\")\n",
    "\n",
    "    if save.lower() == \"yes\":\n",
    "        filepath = input(\"Put the filepath where you want to save the model as following: name_file.sav \")\n",
    "        save_model(to_save=model_trained, filepath=filepath)\n",
    "    \n",
    "    \n",
    "    return model_trained\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Roxan\\\\OneDrive\\\\Documentos\\\\My_map_2\\\\Data-science-bootcamp\\\\Curso\\\\week_10\\\\day1\\\\exercise\\\\train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"0\",1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['2', '4', '6', '7', '8', '12', '13', '15', '17'], dtype='object')\n"
    }
   ],
   "source": [
    "\n",
    "#df_data = df.drop(\"17\", 1)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#.drop(\"17\", 1)\n",
    "\n",
    "objList = df.select_dtypes(include = \"object\").columns\n",
    "print (objList)\n",
    "le = LabelEncoder()\n",
    "\n",
    "for feat in objList:\n",
    "    df[feat] = le.fit_transform(df[feat].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[2.600e+01 1.000e+00 2.000e+00 ... 8.000e+00 7.000e+00 3.047e+03]\n [2.900e+01 0.000e+00 4.000e+00 ... 2.000e+00 2.000e+00 4.426e+03]\n [1.800e+01 3.000e+00 1.300e+01 ... 3.000e+00 3.000e+00 3.766e+03]\n ...\n [3.000e+00 2.000e+00 3.000e+00 ... 4.000e+00 3.000e+00 5.319e+03]\n [1.500e+01 2.000e+00 5.000e+00 ... 4.000e+00 2.000e+00 6.266e+03]\n [1.500e+01 2.000e+00 5.000e+00 ... 1.200e+01 7.000e+00 2.988e+03]]\nSmall train  progress: 0/20:   0%|          | 0/10 [00:00<?, ?it/s]\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-999e18e0cdbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-df71f3b8a41b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchoose_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moption_user\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[0mmodel_trained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-df71f3b8a41b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, df, target_name)\u001b[0m\n\u001b[0;32m    354\u001b[0m                             \u001b[0mmodel_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                             \u001b[0mmodel_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                         \u001b[0mscore_small_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msmall_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1340\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1342\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1343\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1344\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    796\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "main(df=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n0          17.99         10.38          122.80     1001.0          0.11840   \n1          20.57         17.77          132.90     1326.0          0.08474   \n2          19.69         21.25          130.00     1203.0          0.10960   \n3          11.42         20.38           77.58      386.1          0.14250   \n4          20.29         14.34          135.10     1297.0          0.10030   \n..           ...           ...             ...        ...              ...   \n564        21.56         22.39          142.00     1479.0          0.11100   \n565        20.13         28.25          131.20     1261.0          0.09780   \n566        16.60         28.08          108.30      858.1          0.08455   \n567        20.60         29.33          140.10     1265.0          0.11780   \n568         7.76         24.54           47.92      181.0          0.05263   \n\n     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n0             0.27760         0.30010              0.14710         0.2419   \n1             0.07864         0.08690              0.07017         0.1812   \n2             0.15990         0.19740              0.12790         0.2069   \n3             0.28390         0.24140              0.10520         0.2597   \n4             0.13280         0.19800              0.10430         0.1809   \n..                ...             ...                  ...            ...   \n564           0.11590         0.24390              0.13890         0.1726   \n565           0.10340         0.14400              0.09791         0.1752   \n566           0.10230         0.09251              0.05302         0.1590   \n567           0.27700         0.35140              0.15200         0.2397   \n568           0.04362         0.00000              0.00000         0.1587   \n\n     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n0                   0.07871  ...          17.33           184.60      2019.0   \n1                   0.05667  ...          23.41           158.80      1956.0   \n2                   0.05999  ...          25.53           152.50      1709.0   \n3                   0.09744  ...          26.50            98.87       567.7   \n4                   0.05883  ...          16.67           152.20      1575.0   \n..                      ...  ...            ...              ...         ...   \n564                 0.05623  ...          26.40           166.10      2027.0   \n565                 0.05533  ...          38.25           155.00      1731.0   \n566                 0.05648  ...          34.12           126.70      1124.0   \n567                 0.07016  ...          39.42           184.60      1821.0   \n568                 0.05884  ...          30.37            59.16       268.6   \n\n     worst smoothness  worst compactness  worst concavity  \\\n0             0.16220            0.66560           0.7119   \n1             0.12380            0.18660           0.2416   \n2             0.14440            0.42450           0.4504   \n3             0.20980            0.86630           0.6869   \n4             0.13740            0.20500           0.4000   \n..                ...                ...              ...   \n564           0.14100            0.21130           0.4107   \n565           0.11660            0.19220           0.3215   \n566           0.11390            0.30940           0.3403   \n567           0.16500            0.86810           0.9387   \n568           0.08996            0.06444           0.0000   \n\n     worst concave points  worst symmetry  worst fractal dimension  target  \n0                  0.2654          0.4601                  0.11890     0.0  \n1                  0.1860          0.2750                  0.08902     0.0  \n2                  0.2430          0.3613                  0.08758     0.0  \n3                  0.2575          0.6638                  0.17300     0.0  \n4                  0.1625          0.2364                  0.07678     0.0  \n..                    ...             ...                      ...     ...  \n564                0.2216          0.2060                  0.07115     0.0  \n565                0.1628          0.2572                  0.06637     0.0  \n566                0.1418          0.2218                  0.07820     0.0  \n567                0.2650          0.4087                  0.12400     0.0  \n568                0.0000          0.2871                  0.07039     1.0  \n\n[569 rows x 31 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean radius</th>\n      <th>mean texture</th>\n      <th>mean perimeter</th>\n      <th>mean area</th>\n      <th>mean smoothness</th>\n      <th>mean compactness</th>\n      <th>mean concavity</th>\n      <th>mean concave points</th>\n      <th>mean symmetry</th>\n      <th>mean fractal dimension</th>\n      <th>...</th>\n      <th>worst texture</th>\n      <th>worst perimeter</th>\n      <th>worst area</th>\n      <th>worst smoothness</th>\n      <th>worst compactness</th>\n      <th>worst concavity</th>\n      <th>worst concave points</th>\n      <th>worst symmetry</th>\n      <th>worst fractal dimension</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17.99</td>\n      <td>10.38</td>\n      <td>122.80</td>\n      <td>1001.0</td>\n      <td>0.11840</td>\n      <td>0.27760</td>\n      <td>0.30010</td>\n      <td>0.14710</td>\n      <td>0.2419</td>\n      <td>0.07871</td>\n      <td>...</td>\n      <td>17.33</td>\n      <td>184.60</td>\n      <td>2019.0</td>\n      <td>0.16220</td>\n      <td>0.66560</td>\n      <td>0.7119</td>\n      <td>0.2654</td>\n      <td>0.4601</td>\n      <td>0.11890</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.57</td>\n      <td>17.77</td>\n      <td>132.90</td>\n      <td>1326.0</td>\n      <td>0.08474</td>\n      <td>0.07864</td>\n      <td>0.08690</td>\n      <td>0.07017</td>\n      <td>0.1812</td>\n      <td>0.05667</td>\n      <td>...</td>\n      <td>23.41</td>\n      <td>158.80</td>\n      <td>1956.0</td>\n      <td>0.12380</td>\n      <td>0.18660</td>\n      <td>0.2416</td>\n      <td>0.1860</td>\n      <td>0.2750</td>\n      <td>0.08902</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19.69</td>\n      <td>21.25</td>\n      <td>130.00</td>\n      <td>1203.0</td>\n      <td>0.10960</td>\n      <td>0.15990</td>\n      <td>0.19740</td>\n      <td>0.12790</td>\n      <td>0.2069</td>\n      <td>0.05999</td>\n      <td>...</td>\n      <td>25.53</td>\n      <td>152.50</td>\n      <td>1709.0</td>\n      <td>0.14440</td>\n      <td>0.42450</td>\n      <td>0.4504</td>\n      <td>0.2430</td>\n      <td>0.3613</td>\n      <td>0.08758</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.42</td>\n      <td>20.38</td>\n      <td>77.58</td>\n      <td>386.1</td>\n      <td>0.14250</td>\n      <td>0.28390</td>\n      <td>0.24140</td>\n      <td>0.10520</td>\n      <td>0.2597</td>\n      <td>0.09744</td>\n      <td>...</td>\n      <td>26.50</td>\n      <td>98.87</td>\n      <td>567.7</td>\n      <td>0.20980</td>\n      <td>0.86630</td>\n      <td>0.6869</td>\n      <td>0.2575</td>\n      <td>0.6638</td>\n      <td>0.17300</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20.29</td>\n      <td>14.34</td>\n      <td>135.10</td>\n      <td>1297.0</td>\n      <td>0.10030</td>\n      <td>0.13280</td>\n      <td>0.19800</td>\n      <td>0.10430</td>\n      <td>0.1809</td>\n      <td>0.05883</td>\n      <td>...</td>\n      <td>16.67</td>\n      <td>152.20</td>\n      <td>1575.0</td>\n      <td>0.13740</td>\n      <td>0.20500</td>\n      <td>0.4000</td>\n      <td>0.1625</td>\n      <td>0.2364</td>\n      <td>0.07678</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>564</th>\n      <td>21.56</td>\n      <td>22.39</td>\n      <td>142.00</td>\n      <td>1479.0</td>\n      <td>0.11100</td>\n      <td>0.11590</td>\n      <td>0.24390</td>\n      <td>0.13890</td>\n      <td>0.1726</td>\n      <td>0.05623</td>\n      <td>...</td>\n      <td>26.40</td>\n      <td>166.10</td>\n      <td>2027.0</td>\n      <td>0.14100</td>\n      <td>0.21130</td>\n      <td>0.4107</td>\n      <td>0.2216</td>\n      <td>0.2060</td>\n      <td>0.07115</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>565</th>\n      <td>20.13</td>\n      <td>28.25</td>\n      <td>131.20</td>\n      <td>1261.0</td>\n      <td>0.09780</td>\n      <td>0.10340</td>\n      <td>0.14400</td>\n      <td>0.09791</td>\n      <td>0.1752</td>\n      <td>0.05533</td>\n      <td>...</td>\n      <td>38.25</td>\n      <td>155.00</td>\n      <td>1731.0</td>\n      <td>0.11660</td>\n      <td>0.19220</td>\n      <td>0.3215</td>\n      <td>0.1628</td>\n      <td>0.2572</td>\n      <td>0.06637</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>566</th>\n      <td>16.60</td>\n      <td>28.08</td>\n      <td>108.30</td>\n      <td>858.1</td>\n      <td>0.08455</td>\n      <td>0.10230</td>\n      <td>0.09251</td>\n      <td>0.05302</td>\n      <td>0.1590</td>\n      <td>0.05648</td>\n      <td>...</td>\n      <td>34.12</td>\n      <td>126.70</td>\n      <td>1124.0</td>\n      <td>0.11390</td>\n      <td>0.30940</td>\n      <td>0.3403</td>\n      <td>0.1418</td>\n      <td>0.2218</td>\n      <td>0.07820</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>567</th>\n      <td>20.60</td>\n      <td>29.33</td>\n      <td>140.10</td>\n      <td>1265.0</td>\n      <td>0.11780</td>\n      <td>0.27700</td>\n      <td>0.35140</td>\n      <td>0.15200</td>\n      <td>0.2397</td>\n      <td>0.07016</td>\n      <td>...</td>\n      <td>39.42</td>\n      <td>184.60</td>\n      <td>1821.0</td>\n      <td>0.16500</td>\n      <td>0.86810</td>\n      <td>0.9387</td>\n      <td>0.2650</td>\n      <td>0.4087</td>\n      <td>0.12400</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>7.76</td>\n      <td>24.54</td>\n      <td>47.92</td>\n      <td>181.0</td>\n      <td>0.05263</td>\n      <td>0.04362</td>\n      <td>0.00000</td>\n      <td>0.00000</td>\n      <td>0.1587</td>\n      <td>0.05884</td>\n      <td>...</td>\n      <td>30.37</td>\n      <td>59.16</td>\n      <td>268.6</td>\n      <td>0.08996</td>\n      <td>0.06444</td>\n      <td>0.0000</td>\n      <td>0.0000</td>\n      <td>0.2871</td>\n      <td>0.07039</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>569 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets \n",
    "cancer = datasets.load_breast_cancer() \n",
    "\n",
    "df_cancer = pd.DataFrame(data= np.c_[cancer['data'], cancer['target']],\n",
    "                     columns= list(cancer['feature_names']) + ['target'] )\n",
    "\n",
    "df_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}